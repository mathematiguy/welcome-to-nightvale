{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fine-tuning a language model with huggingface.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a0b0fe533ba145c094c1f1ee7836a4a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f31272bc8a20462fa2edefed716eed3e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d2f634c1137c4fe9beaecb6358ab2218","IPY_MODEL_c6fef68740ea494caeef14048adf87a0"]}},"f31272bc8a20462fa2edefed716eed3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2f634c1137c4fe9beaecb6358ab2218":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ef8e6b4e53b243fa8090762535e4c5fb","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1042301,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1042301,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6414fbcac9444772b198146f0587b986"}},"c6fef68740ea494caeef14048adf87a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_21328ee0ab4a4fe7a2c492fd3c350dc8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.04M/1.04M [00:02&lt;00:00, 391kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b279f9d400794dd79d686b9d0c2084dc"}},"ef8e6b4e53b243fa8090762535e4c5fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6414fbcac9444772b198146f0587b986":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"21328ee0ab4a4fe7a2c492fd3c350dc8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b279f9d400794dd79d686b9d0c2084dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9790778a18474f13b6c02ab8591604d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c42121cdec3c4ccbb63b064878689fcd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4cb844a7a3984e51b7069998dcad3a76","IPY_MODEL_41f0eef84f5d4fef9db21f5cb3d2e210"]}},"c42121cdec3c4ccbb63b064878689fcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4cb844a7a3984e51b7069998dcad3a76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1c5fc625d07e4059ad07076da8d8ac83","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f9263845ea87423e9436be3871521e7f"}},"41f0eef84f5d4fef9db21f5cb3d2e210":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1542ed38cb97438d9749ba8fe2f20244","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:01&lt;00:00, 287kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_721c892dc1fb439aa7f2033707ab0c4a"}},"1c5fc625d07e4059ad07076da8d8ac83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f9263845ea87423e9436be3871521e7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1542ed38cb97438d9749ba8fe2f20244":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"721c892dc1fb439aa7f2033707ab0c4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5eb03f3455cb44faa1f8739065d61964":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3c0dc28882b44966ae8ee49281139033","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d526aeec2e0b463593cce943d12c3f2f","IPY_MODEL_0e63f748c98d45e69d2efbd447255137"]}},"3c0dc28882b44966ae8ee49281139033":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d526aeec2e0b463593cce943d12c3f2f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_987da00c01c0487b866def98d2da3222","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1355256,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1355256,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a2c6f5bd0f6d48da8af21c9f0aada0e0"}},"0e63f748c98d45e69d2efbd447255137":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_79cc69dd74604a9784da8455b2eae070","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.36M/1.36M [00:00&lt;00:00, 4.32MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f1c5cba567cb4ebe900e3fcb0ea37629"}},"987da00c01c0487b866def98d2da3222":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a2c6f5bd0f6d48da8af21c9f0aada0e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"79cc69dd74604a9784da8455b2eae070":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f1c5cba567cb4ebe900e3fcb0ea37629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"422672a986c0428d8c609fe28d0d1cf5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f1e8cb82b37343b7a1847afbd81de47b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cfa6c1a42eaa43bfb22dfd765be220b3","IPY_MODEL_d01ed377ac6d49b7bc05292feeee23a8"]}},"f1e8cb82b37343b7a1847afbd81de47b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cfa6c1a42eaa43bfb22dfd765be220b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_263f23f61ad442588c1cbd826fa116a2","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":665,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":665,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b5da490e084049dbac567e41bf625894"}},"d01ed377ac6d49b7bc05292feeee23a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_be7fdb66117f4e968a8176464b207f8a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 665/665 [00:00&lt;00:00, 2.09kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_901980ff57f94349b60e6feb3f338ed3"}},"263f23f61ad442588c1cbd826fa116a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b5da490e084049dbac567e41bf625894":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be7fdb66117f4e968a8176464b207f8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"901980ff57f94349b60e6feb3f338ed3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3e2f3f51ee00455595fe589424a34f9a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d60b0e59bd1b41f0a96b170b28a715aa","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_72dee8267b5e4ed790595035fe94db03","IPY_MODEL_9c6c84f62880455cb4992a46e3510128"]}},"d60b0e59bd1b41f0a96b170b28a715aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"72dee8267b5e4ed790595035fe94db03":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2d2fc70fcc214a8787c716cadcd222f5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":548118077,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":548118077,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_46f136b77f32465485603527858a6303"}},"9c6c84f62880455cb4992a46e3510128":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e5d168672b604530bac2aecc267ec602","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 548M/548M [00:10&lt;00:00, 50.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_455bbde701b9487ea546269f47afa107"}},"2d2fc70fcc214a8787c716cadcd222f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"46f136b77f32465485603527858a6303":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e5d168672b604530bac2aecc267ec602":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"455bbde701b9487ea546269f47afa107":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"TRk9C6TWfwBu","cellView":"form"},"source":["#@title\n","# This block helps the text wrap more nicely in the browser window\n","from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WKhstQQYi1wH"},"source":["# Fine-tuning a language model with huggingface\n"]},{"cell_type":"markdown","metadata":{"id":"VX5jpwpOi9ON"},"source":["This notebook explores generating fake podcast transcripts from \"Welcome to Night Vale\", a favourite podcast of mine from 2012.\n","\n","The source code for this notebook, along with any extra bits I created while I got it working, are available via Github here. Feel free to take a look, although it is much less presentable than this notebook: https://github.com/mathematiguy/welcome-to-nightvale"]},{"cell_type":"markdown","metadata":{"id":"J_quoGtedZpw"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"0Vt1vUjsdZpz"},"source":["## Welcome to Night Vale"]},{"cell_type":"markdown","metadata":{"id":"1Sf-ZfbEdZp2"},"source":["\"Welcome to Night Vale\" is a podcast presented as a radio show for the fictional town of Night Vale, reporting on the strange events that occur within it. It was created in 2012 by Joseph Fink and Jeffrey Cranor, and is full of Lovecraftian cosmic horror with a comical twist.\n","\n","The host of \"Welcome to Night Vale\" is Cecil Baldwin, who is played by American voice actor Cecil Palmer.\n","\n","In this notebook, we will be training a language model to generate fake podcast transcripts from the show using data collected from https://cecilspeaks.tumblr.com/, which contains transcripts for 191 episodes of the show."]},{"cell_type":"markdown","metadata":{"id":"txpatWh8dZp4"},"source":["## Listen to the podcast\n","\n","Listen to an episode for a little while. Try to get a sense of the vibe of the language."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"A6ZrfGW4dZp8","executionInfo":{"status":"ok","timestamp":1625174336385,"user_tz":-720,"elapsed":990,"user":{"displayName":"Kiarie Ndegwa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsrhTjyqh2BsPblZGnbgQrV_XNvzFBUvim2ungyg=s64","userId":"07812276994230817522"}},"outputId":"6c6e597e-955a-441a-bdd9-9899a6291d9f"},"source":["from IPython.display import IFrame\n","\n","IFrame(\"https://www.youtube.com/embed/due3u22Licw\", width=560, height=315)"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","        <iframe\n","            width=\"560\"\n","            height=\"315\"\n","            src=\"https://www.youtube.com/embed/due3u22Licw\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x7fba43f31e10>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"ibOtQSJcdZqE"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"oy2sFY8eNpkM"},"source":["## Get the data\n","\n","To collect the transcripts, I wrote a webscraper using scrapy (https://scrapy.org/) and stored the files in a google drive to be downloaded in the cell below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzB-JSZVIQxN","executionInfo":{"status":"ok","timestamp":1625174344688,"user_tz":-720,"elapsed":7958,"user":{"displayName":"Kiarie Ndegwa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsrhTjyqh2BsPblZGnbgQrV_XNvzFBUvim2ungyg=s64","userId":"07812276994230817522"}},"outputId":"f62c0dba-6449-4623-8bdf-ac32bed9534c"},"source":["# Delete the data if it already exists\n","! rm -rf wtnv.zip transcripts\n","\n","# Download the data\n","! gdown --id \"1szUGhMsH9SFF52AZKRse_gZs7zB7ke8F\"\n","\n","# Unzip the data\n","! unzip wtnv.zip -d transcripts"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1szUGhMsH9SFF52AZKRse_gZs7zB7ke8F\n","To: /content/wtnv.zip\n","\r  0% 0.00/1.24M [00:00<?, ?B/s]\r100% 1.24M/1.24M [00:00<00:00, 81.5MB/s]\n","Archive:  wtnv.zip\n","  inflating: transcripts/1-pilot     \n","  inflating: transcripts/2-glow-cloud  \n","  inflating: transcripts/3-station-management  \n","  inflating: transcripts/4-pta-meeting  \n","  inflating: transcripts/5-the-shape-in-grove-park  \n","  inflating: transcripts/6-the-drawbridge  \n","  inflating: transcripts/7-history-week  \n","  inflating: transcripts/8-the-lights-in-radon-canyon  \n","  inflating: transcripts/9-pyramid   \n","  inflating: transcripts/10-feral-dogs  \n","  inflating: transcripts/11-wheat-amp-wheat-by-products  \n","  inflating: transcripts/12-the-candidate  \n","  inflating: transcripts/13-a-story-about-you  \n","  inflating: transcripts/14-the-man-in-the-tan-jacket  \n","  inflating: transcripts/15-street-cleaning-day  \n","  inflating: transcripts/16-the-phone-call  \n","  inflating: transcripts/17-valentine  \n","  inflating: transcripts/18-the-traveler  \n","  inflating: transcripts/19a-the-sandstorm  \n","  inflating: transcripts/19b-the-sandstorm  \n","  inflating: transcripts/20-poetry-week  \n","  inflating: transcripts/21-a-memory-of-europe  \n","  inflating: transcripts/22-the-whispering-forest  \n","  inflating: transcripts/23-eternal-scouts  \n","  inflating: transcripts/24-the-mayor  \n","  inflating: transcripts/25-one-year-later  \n","  inflating: transcripts/26-faceless-old-woman  \n","  inflating: transcripts/27-first-date  \n","  inflating: transcripts/28-summer-reading-program  \n","  inflating: transcripts/29-subway   \n","  inflating: transcripts/30-dana     \n","  inflating: transcripts/31-a-blinking-light-up-on-the-mountain  \n","  inflating: transcripts/32-yellow-helicopters  \n","  inflating: transcripts/33-cassette  \n","  inflating: transcripts/34-a-beautiful-dream  \n","  inflating: transcripts/35-lazy-day  \n","  inflating: transcripts/36-missing  \n","  inflating: transcripts/37-the-auction  \n","  inflating: transcripts/38-orange-grove  \n","  inflating: transcripts/39-the-woman-from-italy  \n","  inflating: transcripts/40-the-deft-bowman  \n","  inflating: transcripts/41-walk     \n","  inflating: transcripts/42-numbers  \n","  inflating: transcripts/43-visitor  \n","  inflating: transcripts/44-cookies  \n","  inflating: transcripts/45-a-story-about-them  \n","  inflating: transcripts/46-parade-day  \n","  inflating: transcripts/47-company-picnic  \n","  inflating: transcripts/48-renovations  \n","  inflating: transcripts/49-old-oak-doors-part-a  \n","  inflating: transcripts/49-old-oak-doors-part-b  \n","  inflating: transcripts/50-capital-campaign  \n","  inflating: transcripts/51-rumbling  \n","  inflating: transcripts/52-the-retirement-of-pamela-winchell  \n","  inflating: transcripts/53-the-september-monologues  \n","  inflating: transcripts/54-a-carnival-comes-to-town  \n","  inflating: transcripts/55-the-university-of-what-it-is  \n","  inflating: transcripts/56-homecoming  \n","  inflating: transcripts/57-the-list  \n","  inflating: transcripts/58-monolith  \n","  inflating: transcripts/59-antiques  \n","  inflating: transcripts/60-water-failure  \n","  inflating: transcripts/61-briney-depths  \n","  inflating: transcripts/62-hatchets  \n","  inflating: transcripts/63-there-is-no-part-1-part-2  \n","  inflating: transcripts/64-we-must-give-praise  \n","  inflating: transcripts/65-voicemail  \n","  inflating: transcripts/66-worms    \n","  inflating: transcripts/67-best-of  \n","  inflating: transcripts/68-faceless-old-women  \n","  inflating: transcripts/69-fashion-week  \n","  inflating: transcripts/70a-taking-flight  \n","  inflating: transcripts/70b-review  \n","  inflating: transcripts/71-the-registry-of-middle-school-crushes  \n","  inflating: transcripts/72-well-of-night  \n","  inflating: transcripts/73-triptych  \n","  inflating: transcripts/74-civic-changes  \n","  inflating: transcripts/75-through-the-narrow-place  \n","  inflating: transcripts/76-an-epilogue  \n","  inflating: transcripts/77-a-stranger  \n","  inflating: transcripts/78-cooking-stuff-thanksgiving-special  \n","  inflating: transcripts/79-lost-in-the-mail  \n","  inflating: transcripts/80-a-new-sheriff-in-town  \n","  inflating: transcripts/81-after-3327  \n","  inflating: transcripts/82-skating-rink  \n","  inflating: transcripts/83-one-normal-town  \n","  inflating: transcripts/84-past-time  \n","  inflating: transcripts/85-april-monologues  \n","  inflating: transcripts/86-standing-and-breathing  \n","  inflating: transcripts/87-the-trial-of-hiram-mcdaniels  \n","  inflating: transcripts/88-things-fall-apart  \n","  inflating: transcripts/89-whos-a-good-boy-part-1  \n","  inflating: transcripts/90-whos-a-good-boy-part-2  \n","  inflating: transcripts/91-the-1237  \n","  inflating: transcripts/92-if-he-had-lived  \n","  inflating: transcripts/93-big-sister  \n","  inflating: transcripts/94-all-right  \n","  inflating: transcripts/95-zookeeper  \n","  inflating: transcripts/96-negotiations  \n","  inflating: transcripts/97-josefina  \n","  inflating: transcripts/98-flight   \n","  inflating: transcripts/99-michigan  \n","  inflating: transcripts/100-toast   \n","  inflating: transcripts/101-guidelines-for-disposal  \n","  inflating: transcripts/102-love-is-a-shambling-thing  \n","  inflating: transcripts/103-ash-beach  \n","  inflating: transcripts/104-the-hierarchy-of-angels  \n","  inflating: transcripts/105-what-happened-at-the-smithwick-house  \n","  inflating: transcripts/106-filings  \n","  inflating: transcripts/107-the-missing-sky  \n","  inflating: transcripts/108-cal     \n","  inflating: transcripts/109-a-story-about-huntokar  \n","  inflating: transcripts/110-matryoshka  \n","  inflating: transcripts/111-summer-2017-night-vale-usa  \n","  inflating: transcripts/112-citizen-spotlight  \n","  inflating: transcripts/113-niecelet  \n","  inflating: transcripts/114-council-member-flynn-part-1  \n","  inflating: transcripts/115-council-member-flynn-part-2  \n","  inflating: transcripts/116-council-member-flynn-part-3  \n","  inflating: transcripts/117-egemony-part-1-canadian-club  \n","  inflating: transcripts/118-egemony-part-2-the-cavelands  \n","  inflating: transcripts/119-egemony-part-3-love-among-other-things-is-all-you-need  \n","  inflating: transcripts/120-all-smiles-eve  \n","  inflating: transcripts/121-a-story-of-love-and-horror-part-1-barks  \n","  inflating: transcripts/122-a-story-of-love-and-horror-part-2-spire  \n","  inflating: transcripts/123-a-story-of-love-and-horror-part-3-frances  \n","  inflating: transcripts/124-a-door-ajar-part-1  \n","  inflating: transcripts/125-a-door-ajar-part-2  \n","  inflating: transcripts/126-a-door-ajar-part-3  \n","  inflating: transcripts/127-a-matter-of-blood-part-1  \n","  inflating: transcripts/128-a-matter-of-blood-part-2  \n","  inflating: transcripts/129-a-matter-of-blood-part-3  \n","  inflating: transcripts/130-a-story-about-us  \n","  inflating: transcripts/131-brought-to-you-by-kelloggs  \n","  inflating: transcripts/132-bedtime-story  \n","  inflating: transcripts/133-are-you-sure  \n","  inflating: transcripts/134-fall-football-preview  \n","  inflating: transcripts/135-the-mudstone-abyss-part-1  \n","  inflating: transcripts/136-the-mudstone-abyss-part-2  \n","  inflating: transcripts/137-the-mudstone-abyss-part-3  \n","  inflating: transcripts/138-harvest-time  \n","  inflating: transcripts/139-the-birthday-of-lee-marvin  \n","  inflating: transcripts/140-a-blood-stone-carol  \n","  inflating: transcripts/141-save-dark-owl-records  \n","  inflating: transcripts/142-ufo-sighting-report  \n","  inflating: transcripts/143-pioneer-days  \n","  inflating: transcripts/144-the-dreamer  \n","  inflating: transcripts/145-the-veterans  \n","  inflating: transcripts/146-the-birthday-of-lee-marvin  \n","  inflating: transcripts/147-the-protester  \n","  inflating: transcripts/148-the-broadcaster  \n","  inflating: transcripts/149-the-general  \n","  inflating: transcripts/150-the-birthday-of-lee-marvin  \n","  inflating: transcripts/151-the-waterfall  \n","  inflating: transcripts/152-the-great-golden-hand  \n","  inflating: transcripts/153-the-heist-part-1  \n","  inflating: transcripts/154-the-heist-part-2  \n","  inflating: transcripts/155-the-heist-part-3  \n","  inflating: transcripts/156-the-trouble-with-time  \n","  inflating: transcripts/157-the-promise-of-time  \n","  inflating: transcripts/158-the-battle-for-time  \n","  inflating: transcripts/159-cat-show  \n","  inflating: transcripts/160-the-weather  \n","  inflating: transcripts/161-the-space-race  \n","  inflating: transcripts/162-alpha   \n","  inflating: transcripts/163-bravo   \n","  inflating: transcripts/164-the-faceless-old-woman-live  \n","  inflating: transcripts/165-charlie  \n","  inflating: transcripts/166-delta   \n","  inflating: transcripts/167-echo    \n","  inflating: transcripts/168-secret-blotter  \n","  inflating: transcripts/169-the-whittler  \n","  inflating: transcripts/170-to-the-family-and-friends  \n","  inflating: transcripts/171-go-to-the-mirror  \n","  inflating: transcripts/172-return-of-the-obelisk  \n","  inflating: transcripts/173-the-hundred-year-play  \n","  inflating: transcripts/174-radio-jupiter  \n","  inflating: transcripts/175-the-october-monologues  \n","  inflating: transcripts/176-the-autumn-specter  \n","  inflating: transcripts/177-bloody-laws-bloody-claws-the-murder-of-frank-chen  \n","  inflating: transcripts/178-rattlesnake-rest  \n","  inflating: transcripts/179-first-snow  \n","  inflating: transcripts/180-u-view  \n","  inflating: transcripts/181-cs      \n","  inflating: transcripts/182-it-sticks-with-you  \n","  inflating: transcripts/183-the-nephilim  \n","  inflating: transcripts/184-the-fog  \n","  inflating: transcripts/185-fair    \n","  inflating: transcripts/186-the-many-lives-of-frank-chen  \n","  inflating: transcripts/187-citizen-spotlight-the-spire  \n","  inflating: transcripts/188-listener-questions  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hFPuxGhUdZqL"},"source":["## Take a peek at a transcript"]},{"cell_type":"markdown","metadata":{"id":"WDF3f7FMdZqN"},"source":["We have just downloaded 191 podcast transcripts, which are listed above. Next we can inspect one of the files using `head`, which displays the first 10 lines of a file.\n","\n","If you would like to see the rest of the file, or more files, then feel free to explore the `transcripts/` directory on the left."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMQLEeLmNnog","executionInfo":{"status":"ok","timestamp":1625174358867,"user_tz":-720,"elapsed":497,"user":{"displayName":"Kiarie Ndegwa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsrhTjyqh2BsPblZGnbgQrV_XNvzFBUvim2ungyg=s64","userId":"07812276994230817522"}},"outputId":"13bebc23-5c59-4107-8796-df55e1cd0771"},"source":["# Look at the data\n","! head transcripts/1-pilot"],"execution_count":3,"outputs":[{"output_type":"stream","text":["1 - Pilot\n","\n","A friendly desert community, where the sun is hot, the moon is beautiful, and mysterious lights pass overhead while we all pretend to sleep. Welcome to Night Vale. \n","\n","Hello listeners. To start things off, I’ve been asked to read to read this brief notice. The City Council announces the opening of a new dog park at the corner of Earl and Summerset, near the Ralphs. They would like to remind everyone that dogs are not allowed in the dog park. People are not allowed in the dog park. It is possible you will see hooded figures in the dog park. Do not approach them. Do not approach the dog park. The fence is electrified and highly dangerous. Try not to look at the dog park and especially do not look for any period of time at the hooded figures. The dog park will not harm you.\n","\n","And now the news. Old Woman Josie, out near the car lot, says the Angels revealed themselves to her. Said they were ten feet tall, radiant, one of them was black. Said they helped her with various household chores. One of them changed  a light bulb for her, the porch light. She’s offering to sell the old light bulb, which has been touched by an angel (it was the black angel, if that sweetens the pot for anyone). If you’re interested, contact Old Woman Josie. She’s out near the car lot.\n","\n","A new man came in to town today.  Who is he? What does he want from us? Why his perfect and beautiful haircut? Why his perfect and beautiful coat? He says he is a scientist. Well, we have all been scientists at one point or another in our lives.  But why now? Why here? And just what does he plan to do with all those beakers and humming electrical instruments in that lab he’s renting, the one next to Big Rico’s Pizza. No one does a slice, like Big Rico. No one.  \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FMaBs0-WdZqQ"},"source":["## Listen to the transcript"]},{"cell_type":"markdown","metadata":{"id":"uA1TU8lWdZqQ"},"source":["Here's another iFrame with episode 1 in it so you can compare the recording to the transcript above:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"ARUXLQnHOSwM","executionInfo":{"status":"ok","timestamp":1625174363085,"user_tz":-720,"elapsed":404,"user":{"displayName":"Kiarie Ndegwa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsrhTjyqh2BsPblZGnbgQrV_XNvzFBUvim2ungyg=s64","userId":"07812276994230817522"}},"outputId":"d85ba7b3-1fa7-4340-8905-a665c0ad2cf6"},"source":["IFrame(\"https://www.youtube.com/embed/Ujksjzqrhys\", width=560, height=315)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","        <iframe\n","            width=\"560\"\n","            height=\"315\"\n","            src=\"https://www.youtube.com/embed/Ujksjzqrhys\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x7fba3dec30d0>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"z3Y0JHoAdZqS"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"blPjgxcrPTLR"},"source":["## Split the data into train/test sets\n","\n","Splitting data into different sets is a common operation for training any machine learning model. Because the model will learn from data, we need to set aside some data that we will not show the language model for evaluation. This helps us to detect overfitting.\n","\n","There is a trade-off between providing more training data, and avoiding overfitting. The more training data you provide, the better your model performs. But the smaller your test set, the more likely you are to overfit.\n","\n","For this project, we'll take the first 90% of the podcast transcripts and add them to a training set, while the remaining 10% are written to a test set. This decision is somewhat arbitrary, and you could argue for putting almost everything in the training set for this application.\n","\n","### How we chose to split the data\n","\n","There are 191 transcripts, so we can take the first 171 and pipe them to file which we'll call `train.txt`, and take the remaining 20 transcripts and pipe them to `test.txt`.\n","\n","In the cell below, I used `bash` command to concatenate the training + test text files automatically.\n","\n","### An unnecessary bash diversion\n","\n","If you wanna know how this works at a high level, here's a summary (but feel free to ignore it):\n","\n","- `ls` lists the files in the `transcripts` directory, `-t` orders the files in sequence.\n","- Then we use `head` to grab the first 171 files for the training set, or in the latter case, we use `tail` to grab to last 20 files\n","- Then `xargs` is a bit like a for loop. It runs `cat` over each file, which prints the file contents to stdout.\n","- Then we pipe the file contents to the output file (`train.txt` or `test.txt`) using the `>` symbol."]},{"cell_type":"code","metadata":{"id":"3wVZXcbKSsJC","executionInfo":{"status":"ok","timestamp":1625174370024,"user_tz":-720,"elapsed":717,"user":{"displayName":"Kiarie Ndegwa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsrhTjyqh2BsPblZGnbgQrV_XNvzFBUvim2ungyg=s64","userId":"07812276994230817522"}}},"source":["p# Save the first 171 transcripts to train.txt\n","! ls transcripts -t | head -n 171 | xargs -I {} cat transcripts/{} > train.txt\n","\n","# Save the last 20 transcripts to train.txt\n","! ls transcripts -t | tail -n 20 | xargs -I {} cat transcripts/{} > test.txt"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OjQLrMwedZqU"},"source":["Once this cell has been run, the files `train.txt` and `test.txt` should now exist. We will inspect them in the next step just to make sure we did it right, but you can always check them directly using the panel on the left."]},{"cell_type":"markdown","metadata":{"id":"6hpBmUz7dZqU"},"source":["## Inspect the training + test data\n","\n","We can take a quick look at `train.txt` and `test.txt`, and count the number of lines in each. Alternatively open the files in a separate tab/window and look at them for yourself."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-AiVoXndZqU","executionInfo":{"status":"ok","timestamp":1625174379258,"user_tz":-720,"elapsed":700,"user":{"displayName":"Kiarie Ndegwa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsrhTjyqh2BsPblZGnbgQrV_XNvzFBUvim2ungyg=s64","userId":"07812276994230817522"}},"outputId":"aa407162-b1d8-4417-b8ed-8db724befdfc"},"source":["# Show the first 10 lines of train.txt\n","! head train.txt"],"execution_count":7,"outputs":[{"output_type":"stream","text":["1 - Pilot\n","\n","A friendly desert community, where the sun is hot, the moon is beautiful, and mysterious lights pass overhead while we all pretend to sleep. Welcome to Night Vale. \n","\n","Hello listeners. To start things off, I’ve been asked to read to read this brief notice. The City Council announces the opening of a new dog park at the corner of Earl and Summerset, near the Ralphs. They would like to remind everyone that dogs are not allowed in the dog park. People are not allowed in the dog park. It is possible you will see hooded figures in the dog park. Do not approach them. Do not approach the dog park. The fence is electrified and highly dangerous. Try not to look at the dog park and especially do not look for any period of time at the hooded figures. The dog park will not harm you.\n","\n","And now the news. Old Woman Josie, out near the car lot, says the Angels revealed themselves to her. Said they were ten feet tall, radiant, one of them was black. Said they helped her with various household chores. One of them changed  a light bulb for her, the porch light. She’s offering to sell the old light bulb, which has been touched by an angel (it was the black angel, if that sweetens the pot for anyone). If you’re interested, contact Old Woman Josie. She’s out near the car lot.\n","\n","A new man came in to town today.  Who is he? What does he want from us? Why his perfect and beautiful haircut? Why his perfect and beautiful coat? He says he is a scientist. Well, we have all been scientists at one point or another in our lives.  But why now? Why here? And just what does he plan to do with all those beakers and humming electrical instruments in that lab he’s renting, the one next to Big Rico’s Pizza. No one does a slice, like Big Rico. No one.  \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_42qJWjQdZqW","executionInfo":{"status":"ok","timestamp":1625174383212,"user_tz":-720,"elapsed":626,"user":{"displayName":"Kiarie Ndegwa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsrhTjyqh2BsPblZGnbgQrV_XNvzFBUvim2ungyg=s64","userId":"07812276994230817522"}},"outputId":"edd4bf18-ee05-46aa-ed8b-49591d2616d4"},"source":["# Show the first 10 lines of test.txt\n","! head test.txt"],"execution_count":8,"outputs":[{"output_type":"stream","text":["169 - The Whittler\n","\n","[\n","\n","]\n","\n","Let us go then you and I,\n","\n","when the evening is spread out against the sky,\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpN4Bv_jdZqW","executionInfo":{"status":"ok","timestamp":1625174390013,"user_tz":-720,"elapsed":483,"user":{"displayName":"Kiarie Ndegwa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsrhTjyqh2BsPblZGnbgQrV_XNvzFBUvim2ungyg=s64","userId":"07812276994230817522"}},"outputId":"ac05220a-a13a-4427-e7dc-fed2afb8d52d"},"source":["# Show the number of lines in train.txt and test.txt\n","! wc -l train.txt test.txt"],"execution_count":9,"outputs":[{"output_type":"stream","text":["  23928 train.txt\n","   3008 test.txt\n","  26936 total\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1hl5HqqMdZqX"},"source":["### Train test split completed!\n","\n","Now we have successfully prepared our dataset for training, we are ready to start building our fine-tuning pipeline."]},{"cell_type":"markdown","metadata":{"id":"jABPUFUQdZqY"},"source":["----"]},{"cell_type":"markdown","metadata":{"id":"sO4xXZR7Tgl-"},"source":["## Fine-tuning a language model"]},{"cell_type":"markdown","metadata":{"id":"NBeTAk0wdZqY"},"source":["In this step, we are going to fine-tune a language model using the `huggingface` (https://huggingface.co) team's excellent `transformers` package.\n","\n","`transformers` allows you to download, use and manipulate a wide range of language models trained for different applications. They provide tools for downloading model weights, using them for inference, training from scratch and fine-tuning. It's also pretty easy to use.\n","\n","More information about `transformers` is available here: https://huggingface.co/transformers/"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":764},"id":"hb72xHlQdlBI","executionInfo":{"status":"ok","timestamp":1625151377636,"user_tz":-720,"elapsed":6265,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"e8011e39-dc5a-4842-ded0-1b492452a40a"},"source":["# Install the transformers python module\n","! pip install transformers[torch]"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting transformers[torch]\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 9.3MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[torch]) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[torch]) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers[torch]) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[torch]) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 46.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[torch]) (2.23.0)\n","Collecting huggingface-hub==0.0.12\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers[torch]) (3.13)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[torch]) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 36.4MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers[torch]) (4.5.0)\n","Requirement already satisfied: torch>=1.0; extra == \"torch\" in /usr/local/lib/python3.7/dist-packages (from transformers[torch]) (1.9.0+cu102)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers[torch]) (2.4.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]) (2021.5.30)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers[torch]) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers[torch]) (3.4.1)\n","Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OzBEQKozdZqY"},"source":["## GPT-2\n","\n","We are going to be using the `GPT-2` language model, which was pre-trained by Open AI who first published it in 2019. You can read more about `GPT-2` here: https://openai.com/blog/gpt-2-1-5b-release/\n","\n","We are going to use the `GPT2Tokenizer`, and `GPT2LMHeadModel` tools.\n","- A `tokenizer` breaks up a string of text into words or word-fragments in a way that the language model understands.\n","- An `LMHeadModel` exposes the Language Model of the GPT-2 model, which makes text generation possible.\n","\n","There are other kinds of Language Models which are specialised for different tasks. You can find a bunch of summaries for all the models supported by `transformers` here: https://huggingface.co/transformers/model_summary.html"]},{"cell_type":"code","metadata":{"id":"2sDObY9mTij-","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1625151383734,"user_tz":-720,"elapsed":6107,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"7b5613c7-0cd7-4e30-86d2-74041ac6b925"},"source":["# Import transformers stuff\n","from transformers import (\n","    GPT2Tokenizer,\n","    GPT2LMHeadModel,\n","    TextDataset,\n","    DataCollatorForLanguageModeling,\n","    Trainer,\n","    TrainingArguments,\n",")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"0wnRlvx0TtP3"},"source":["### Load the pre-trained GPT-2 model and Tokenizer\n","\n","Now that we have imported the library, we need to download the `GPT-2` model weights so we can start using them. The cell below downloads them straight to disk from `huggingface`'s servers."]},{"cell_type":"code","metadata":{"id":"qvHHwRgkTpgC","colab":{"base_uri":"https://localhost:8080/","height":266,"referenced_widgets":["a0b0fe533ba145c094c1f1ee7836a4a1","f31272bc8a20462fa2edefed716eed3e","d2f634c1137c4fe9beaecb6358ab2218","c6fef68740ea494caeef14048adf87a0","ef8e6b4e53b243fa8090762535e4c5fb","6414fbcac9444772b198146f0587b986","21328ee0ab4a4fe7a2c492fd3c350dc8","b279f9d400794dd79d686b9d0c2084dc","9790778a18474f13b6c02ab8591604d5","c42121cdec3c4ccbb63b064878689fcd","4cb844a7a3984e51b7069998dcad3a76","41f0eef84f5d4fef9db21f5cb3d2e210","1c5fc625d07e4059ad07076da8d8ac83","f9263845ea87423e9436be3871521e7f","1542ed38cb97438d9749ba8fe2f20244","721c892dc1fb439aa7f2033707ab0c4a","5eb03f3455cb44faa1f8739065d61964","3c0dc28882b44966ae8ee49281139033","d526aeec2e0b463593cce943d12c3f2f","0e63f748c98d45e69d2efbd447255137","987da00c01c0487b866def98d2da3222","a2c6f5bd0f6d48da8af21c9f0aada0e0","79cc69dd74604a9784da8455b2eae070","f1c5cba567cb4ebe900e3fcb0ea37629","422672a986c0428d8c609fe28d0d1cf5","f1e8cb82b37343b7a1847afbd81de47b","cfa6c1a42eaa43bfb22dfd765be220b3","d01ed377ac6d49b7bc05292feeee23a8","263f23f61ad442588c1cbd826fa116a2","b5da490e084049dbac567e41bf625894","be7fdb66117f4e968a8176464b207f8a","901980ff57f94349b60e6feb3f338ed3","3e2f3f51ee00455595fe589424a34f9a","d60b0e59bd1b41f0a96b170b28a715aa","72dee8267b5e4ed790595035fe94db03","9c6c84f62880455cb4992a46e3510128","2d2fc70fcc214a8787c716cadcd222f5","46f136b77f32465485603527858a6303","e5d168672b604530bac2aecc267ec602","455bbde701b9487ea546269f47afa107"]},"executionInfo":{"status":"ok","timestamp":1625151400187,"user_tz":-720,"elapsed":16465,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"edfc3e70-23c3-4a56-839c-954d16be903e"},"source":["# Load a tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Load GPT2 model\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0b0fe533ba145c094c1f1ee7836a4a1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9790778a18474f13b6c02ab8591604d5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5eb03f3455cb44faa1f8739065d61964","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"422672a986c0428d8c609fe28d0d1cf5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e2f3f51ee00455595fe589424a34f9a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OaY1NYVEUAG6"},"source":["## Load our training datasets\n","\n","Next, we need to load our train + test datasets into a format that the `GPT-2` can use. Notice we pass the `train.txt` and `test.txt` files here."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"PdxI_4ykTzGu","executionInfo":{"status":"ok","timestamp":1625151403647,"user_tz":-720,"elapsed":3464,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"32450e98-abc3-4dce-f6cc-70e2b8ff91ff"},"source":["train_dataset = TextDataset(\n","    tokenizer=tokenizer, file_path='train.txt', block_size=128\n",")\n","\n","test_dataset = TextDataset(tokenizer=tokenizer, file_path='test.txt', block_size=128)\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False, # GPT-2 only supports mlm=False\n",")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"59OGsy7fdZqa"},"source":["## Setting hyperparameters\n","\n","In the cell below, we set hyperparameters which will affect how our model will run. This table gives a summary of what each hyperparameter does, and how it affects the model if you set it too high or too low.\n","\n","| Hyperparameter name | What it does | What if its too big | What if its too small |\n","| --- | --- | --- | --- |\n","| `num_train_epochs` | The number of times the language model will review the text during training. | The model takes a long time to train. | The model will not learn much and the results will lean towards the pre-trained model instead of the new data you have provided. |\n","| `per_device_train_batch_size` | The size of the batches in which the training loop will consume the training data. | You will run out of GPU memory. | The train will take a long time. |\n","| `per_device_eval_batch_size` | The size of the batches in which the training loop will consume the test data. | You will run out of GPU memory. | The model evaluation will take a long time. |\n","| `eval_steps` | Sets how frequently the model will run an evaluation step (that is, it will run inference on the test data to check for overfitting). | You will run evaluation too often which will slow down the training loop. | You may overfit by a lot before you are able to notice. |\n","| `save_steps` | Sets how frequently the model will write checkpoints to disk in order to save its progress. | If your train fails you will need to re-run a lot of computation. | You will create too many checkpoints and run out of disk space. |\n","\n","In actuality, there are _many_ more hyperparameters than this. You can read more about them here: https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments\n","\n","Now we set the hyperparameter values in the cell below:"]},{"cell_type":"code","metadata":{"id":"PFO4mAxQUk4z","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1625151403655,"user_tz":-720,"elapsed":18,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"59f7961a-c99c-4549-d09f-ca9cf38f443e"},"source":["# You can set this higher, but it will take longer\n","num_train_epochs = 1 #@param {type:\"integer\"}\n","\n","# Mostly these can be left as they are - feel free to play with them and see what happens however\n","per_device_train_batch_size = 24 #@param {type:\"integer\"}\n","per_device_eval_batch_size = 16  #@param {type:\"integer\"}\n","eval_steps = 400                 #@param {type:\"integer\"}\n","save_steps = 800                 #@param {type:\"integer\"} \n","\n","training_args = TrainingArguments(\n","    output_dir='wtnv_model',                                 # The folder where we save the model\n","    overwrite_output_dir=True,                               # overwrite the content of the output directory\n","    num_train_epochs=num_train_epochs,                       # number of training epochs\n","    per_device_train_batch_size=per_device_train_batch_size, # batch size for training\n","    per_device_eval_batch_size=per_device_eval_batch_size,   # batch size for evaluation\n","    eval_steps=eval_steps,                                   # Number of update steps between two evaluations.\n","    save_steps=save_steps,                                   # after # steps model is saved\n","    prediction_loss_only=True,\n",")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"ebv42GzjdZqb"},"source":["## Fine-tune the model\n","\n","Now we have done all that work, we can finally fine-tune our model."]},{"cell_type":"code","metadata":{"id":"PxvZT5sLUt9n","colab":{"base_uri":"https://localhost:8080/","height":342},"executionInfo":{"status":"ok","timestamp":1625151587686,"user_tz":-720,"elapsed":184044,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"91e12c4d-e3e9-4fae-c9ee-85c9e822536c"},"source":["# Initialise a Trainer object\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")\n","\n","# Run the model train\n","trainer.train()\n","\n","# Save the model when it's done\n","trainer.save_model()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running training *****\n","  Num examples = 4559\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 190\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [190/190 02:49, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to wtnv_model\n","Configuration saved in wtnv_model/config.json\n","Model weights saved in wtnv_model/pytorch_model.bin\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"hvzyTNLVYuBB"},"source":["## Generate some text\n","\n","Now that we have trained a model, it's time to load it into memory and run some text through it to see the results for ourself.\n","\n","In order to generate text, we need to do the following:\n","\n","- Load the trained model into memory\n","- Feed some text to start the new transcript\n","- Convert the text to a vector of token IDs that the model understands"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":817},"id":"yfQ_S0dOYu5U","executionInfo":{"status":"ok","timestamp":1625151589302,"user_tz":-720,"elapsed":1626,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"ace2b2e5-66e3-4565-e1c3-7f2930758590"},"source":["# Load the model into memory\n","wtnv_model = GPT2LMHeadModel.from_pretrained(\"wtnv_model\", local_files_only=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["loading configuration file wtnv_model/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"gradient_checkpointing\": false,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.8.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading weights file wtnv_model/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at wtnv_model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"cUoXsUkOdZqd"},"source":["If you want the new transcript to start with different text, change the `seed_text` variable:"]},{"cell_type":"code","metadata":{"id":"mTX_ZG9FdZqd","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1625152089623,"user_tz":-720,"elapsed":314,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"84cacae7-7718-4e11-fe1a-62bb3b87f1be"},"source":["# Feed some text to start the new transcript\n","seed_text = 'The glow cloud'  #@param {type:\"string\"}"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Cs2fRMo_dZqd"},"source":["`seed_ids` is a vector of IDs which represent the words in the source sentence.\n","\n","The `tokenizer` object provides a map which converts words to IDs and IDs back to words. The number of IDs should match the number of words in the `seed_text`, and if you use the same word more than once the IDs should match."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"tAN-WzlPdZqe","executionInfo":{"status":"ok","timestamp":1625152091786,"user_tz":-720,"elapsed":466,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"e1eecb5a-599e-44dc-fa7a-e1fedc331b65"},"source":["# Convert the sentence to a tensor of token IDs\n","seed_ids = tokenizer.encode(seed_text, return_tensors='pt')\n","print(seed_ids)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["tensor([[  464, 19634,  6279]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NdeVunMadZqe"},"source":["You can decode the `input_ids` using the `tokenizer.decode` methods as follows:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ikGUAat0dZqe","executionInfo":{"status":"ok","timestamp":1625152092301,"user_tz":-720,"elapsed":3,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"a891c578-09b5-41f5-df1a-cc4a3c81ebd6"},"source":["# Convert the seed_ids back to text\n","print(tokenizer.decode(seed_ids[0]))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["The glow cloud\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QJeknTnSdZqf"},"source":["In the following cell we will actually generate text using the new model's `generate` method. There are a lot of interesting details between how sampling language models work, but the general idea is that the language model provides probabilities for next words given the words so far.\n","\n","Then you sample these probabilities using one of a number of strategies, and the way you sample these probabilities affects the text that comes out at the end.\n","\n","For more information on how `generate` works, take a look at this blog post: https://huggingface.co/blog/how-to-generate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"a7seigeZY1Tt","executionInfo":{"status":"ok","timestamp":1625152163026,"user_tz":-720,"elapsed":68660,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"ae2aaf00-2cba-499d-82aa-27ede158bc44"},"source":["output = wtnv_model.generate(\n","    \n","    seed_ids,       # The seed text IDs\n","    do_sample=True,\n","    \n","    # Tweak these values and see what they do:\n","     max_length=1000  #@param {type:\"integer\"}\n","    ,top_k=0          #@param {type:\"integer\"}\n","    ,top_p=0.92       #@param {type:\"number\"}\n","    ,temperature=0.9  #@param {type:\"number\"}\n","\n",")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2m5f-tMJcD2F","executionInfo":{"status":"ok","timestamp":1625152163027,"user_tz":-720,"elapsed":9,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"176c03c0-f41a-43d4-9bd0-472dcd902e50"},"source":["print(tokenizer.decode(output[0]))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["The glow cloud was a terrible, terrible government order, and we don’t understand how they put up with it. \n","\n","But we did. And we have to keep going. Our search for answers continues apace. We will keep looking. \n","\n","In the meantime, the moon is still glinting bright from some unseen source. The strange entities who feed it whisper that it might be a moon of some kind. They say the moon is a kind of dandelion, and we have to keep looking. We must keep looking. And as we try to find answers to this day, as we wait in complete silence to hear the voices that say, “We found you, Cecil.” \n","\n","Stay tuned next for an unknown source, lurking in your home, whispering in your ear, whispering in your stomach, whispering in your mouth, whispering in your ears.\n","\n","And Good night, Night Vale. Good night.\n","\n","###\n","\n","PROVERB: Cheapest steak in the world. Ugh, nevermind. Cheapest steak in the world. 99 - The Pilot\n","\n","[\n","\n","]\n","\n","[overlapping]\n","\n","Frances Donaldson is a retired postal carrier. She lives in an apartment, and occasionally sits at the kitchen table, on her lap, wondering if she should call home and finally deciding it's time to take a break and go home. She doesn’t know what she is doing. She doesn’t want to find out. She doesn’t want to hear what she is feeling. She is tired of waiting for what she wants and it is what she wants. Welcome to Night Vale. \n","\n","###\n","\n","Frances is an elderly woman, living in an apartment and sometimes sitting at the kitchen table, on her lap, wondering if she should call home and finally deciding it’s time to take a break and go home. She doesn’t know what she is doing. She doesn’t want to find out. She wants to know everything and everything is coming from her. Everything is coming from her. \n","\n","She lives in an apartment. It was her apartment, and she spent most of her childhood with someone else. It was the same apartment as the one she lived in, but it was different, and she had to spend time with someone else who knew her. Or else. It was the same apartment.\n","\n","She stayed here long after her apartment had been taken over by someone who was not her. And for the first time in her life, she felt alone, feeling sad. She didn’t know why she wanted to leave. She wanted to make herself feel welcome. To be alone. To feel loved. And to be alone. She wanted to be alone. And she wanted to be alone. This alone time was normal, so she didn’t care. She had a plan. She planned it.\n","\n","She found a car and drove away. She drove much faster than she had planned. She made no sense of it. She didn’t care about anything else. She wasn’t here to prove her plans. She was only doing what she wanted.\n","\n","She put on a mask and drove away. She had no idea where she was going or where she was going. It was a different part of her life. She had been here long before she had any other plans. She drove away soon after. She had to do it, for no other reason than she didn’t want to live.\n","\n","Frances was always lonely. She did not like to be lonely. She hated loneliness. She did not like to be alone. She wanted to be alone. She wanted to feel alone.\n","\n","And she wanted to be alone. But she didn’t want to be alone. She wanted to feel alone. She wanted to be alone. It was so hard to find home. It was so hard to find home. She wanted to feel alone.\n","\n","Frances felt lonely. She felt empty. She felt nothing.\n","\n","Frances wanted to feel alone. She wanted to feel alone. She wanted to feel alone. \n","\n","But the most she could do was feel some sadness. That was all she needed to be happy. But there was no way to feel joy. It was all she needed to be happy. She did not want to be happy. She had forgotten how to feel joy. She had forgotten how to feel sadness.\n","\n","Frances felt empty. She felt empty. \n","\n","And she wanted to feel empty. She wanted to feel empty. But the most she could do was feel some sadness. That was all she needed to be happy. She had forgotten how to feel sadness. She had forgotten everything and wanted to feel nothing.\n","\n","Frances felt empty.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ThIj1aY3dZqh"},"source":["## Here's one I prepared earlier"]},{"cell_type":"markdown","metadata":{"id":"GM2GgOhvdZqh"},"source":["Now, we can download a 20 epoch model train that I ran separately on the same data. It took about 20 minutes to train on an Nvidia RTX 2080 Ti GPU. Because this one has been trained considerably longer, the results should be noticably different.\n","\n","The model weights are about 500MB zipped, so downloading it will just a few seconds."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"id":"dD8CB1GrdZqh","executionInfo":{"status":"ok","timestamp":1625151695360,"user_tz":-720,"elapsed":14065,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"4168d9fa-c5fc-4955-c1e1-2ba566f89455"},"source":["# Delete the model if it exists\n","! rm -rf wtnv20_model\n","\n","# Download the model\n","! gdown --id \"1igtAPMjk-fDFCSC2BS_W3FyPyJSK2FSJ\"\n","\n","# Unzip the model\n","! unzip wtnv20_model.zip -d wtnv20_model"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1igtAPMjk-fDFCSC2BS_W3FyPyJSK2FSJ\n","To: /content/wtnv20_model.zip\n","463MB [00:05, 81.5MB/s]\n","Archive:  wtnv20_model.zip\n","  inflating: wtnv20_model/config.json  \n","  inflating: wtnv20_model/pytorch_model.bin  \n","  inflating: wtnv20_model/training_args.bin  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bTf7QH94dZqi"},"source":["Now we can quickly re-initialise the new model and generate another transcript."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":835},"id":"EafMYf4QdZqi","executionInfo":{"status":"ok","timestamp":1625152051131,"user_tz":-720,"elapsed":71069,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"fe35ee7f-e898-4c20-898d-7fb8751b2d30"},"source":["# Load the 20-epoch model into memory\n","wtnv20_model = GPT2LMHeadModel.from_pretrained(\"wtnv20_model\", local_files_only=True)\n","\n","# Feed some text to start the new transcript\n","seed_text = 'The dog park'  #@param {type:\"string\"}\n","\n","# Convert the sentence to a tensor of token IDs\n","seed_ids = tokenizer.encode(seed_text, return_tensors='pt')\n","\n","# Generate output\n","output = wtnv20_model.generate(\n","    seed_ids,       # The seed text IDs\n","    do_sample=True\n","    \n","    # Tweak these values and see what they do:\n","    ,max_length=1000   #@param {type:\"integer\"}\n","    ,top_k=0          #@param {type:\"integer\"}\n","    ,top_p=0.92       #@param {type:\"number\"}\n","    ,temperature=0.9  #@param {type:\"number\"}\n",")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["loading configuration file wtnv20_model/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"gradient_checkpointing\": false,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.8.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading weights file wtnv20_model/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at wtnv20_model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"B4JMy564hHwn","executionInfo":{"status":"ok","timestamp":1625152051134,"user_tz":-720,"elapsed":8,"user":{"displayName":"Caleb Moses","photoUrl":"","userId":"05812186704682785223"}},"outputId":"90f11943-bbc2-45ce-d02c-62ad646b5734"},"source":["print(tokenizer.decode(output[0]))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["The dog park will be closed to dogs, and all Night Vale citizens are advised to stay home from the dog park. Please make sure to bring your own litter. See you there.\n","\n","#####\n","\n","An update on the dog park situation. The man in the tan jacket was spotted by several joggers today. They said they saw him in the shadows around the Dog Park. One of them added that they saw a man holding a hooded mannequin and a deer skin suitcase, and that he looked nice. Another added \"He looks like a good guy. Good dad, right?\" and \"Mmm. It's been a while.\" Another added \"You look very nice.\" And, finally, a statement from the dog park spokesperson: \"Our Park Management assures us that the Dog Park is fully operational and all municipal dog parks are completely free of dogs.\"\n","\n","He then added that there was a heavy security presence at the dog park. He then added \"Please not bring any pets into or out of the Dog Park. The dog park will not cause any harm. The dog park policy is dog friendly.\"\n","\n","#####\n","\n","But first a word from our sponsor. Today's show is brought to you by Dark Owl Records, the world's leading music retailer. Dark Owl has a long history of producing great music, and we wanted to bring you our newest album. \n","\n","Take a listen to this album. It’s so full of soul and soul and soul, and every chord and beat is totally your own. It’s so full of soul and soul, and every beat is completely your own.\n","\n","We recorded it in a studio that used a desk lamp and a pair of jeans, so when we went to make the album, we had to throw in a few more instruments and some motion sensors.\n","\n","But the album is so true to the music of our music lovers. We put music and lyrics in a cello, but it’s a live recording, not a spoken word song. It is not a visual metaphor.\n","\n","Instead we recorded it in one of our studio's many metal cams, inside an old shed. The microphones in the shed are made of...I don’t know. Some sort of magnetic fence, although that would be nice. Or maybe other kinds of electrified fences. We recorded it in a completely sealed room, because it seemed like the only way to get in and out of there safely. But other than that, it seemed...well, it was very quiet. \n","\n","The first thing we did was move the microphone from left to right, a three dimensional grid of seven parallel voices. We recorded the album in a single voice, but then the mic rotated so that it was in a different place in the recording than it was in the album before. Then when we were finished, we switched places so that we could hear each other more clearly.\n","\n","The album is one of our most celebrated recordings and is now available for listening in stores all over Night Vale. \n","\n","The staff there is sweating profusely and they want you to take a listen to it right now.\n","\n","Night Vale, our show is now available in your favorite radio station. \n","\n","Take a listen to it. Do you remember it well? \n","\n","You’re listening to it, Cecil. \n","\n","It’s called “Stolen Soul” by The Birthday Boys.\n","\n","I guess you’re listening too. Listeners, it’s stolen.\n","\n","I don’t care. Listeners, I’m still listening. Listeners, it’s yours.\n","\n","[pause]\n","\n","[pause]\n","\n","[pause]\n","\n","Listeners, did you hear that old adage about the box being a thousand years old? Not that old, but not that old. The box was...it wasn’t a box. It was a thing hidden in an old storage closet in a cornfield in a desert otherworld. I have no idea what that thing was. Or why it kept coming back. \n","\n","The box was a soft green plastic sheet. It looked just like an old refrigerator that came along. The only change was that it had a long metal zipper and was attached to a heavy chain. I am unsure of its purpose, but I did see this. Inside was...well...a box. \n","\n","I am uncertain of its purpose. It moved, moved. It moved. I do not know the purpose of its movement, but I am certain it moved. I do not know the language of its movement, but I am certain it moved.\n","\n","The box was an empty square, but it moved. It moved. I do not know the language of its movement, but I am certain it moved\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ckaZKT_GdZqi"},"source":["## More things to try:\n","\n","- Google some of the characters and places to see if they exist in the world of Welcome to Night Vale or not\n","- Take some time to generate new texts with different variables and get a feel for how the output changes.\n","- Make some comments on the weaknesses of the model output.\n","- See if you can make a transcript that is amusingly terrible\n","- See if you can make a transcript that you think is pretty good"]}]}