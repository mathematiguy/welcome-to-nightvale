{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiV2SShmISek"
   },
   "source": [
    "# Fine-tuning a language model with huggingface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores generating fake podcast transcripts from \"Welcome to Night Vale\", a favourite podcast of mine from 2012.\n",
    "\n",
    "The source code for this notebook, along with any extra bits I created while I got it working, are available via Github here. Feel free to take a look, although it is much less presentable than this notebook: https://github.com/mathematiguy/welcome-to-nightvale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown==3.13.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (3.13.0)\n",
      "Requirement already satisfied: torch==1.8.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.8.1+cu111)\n",
      "Requirement already satisfied: transformers==4.6.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (4.6.1)\n",
      "Requirement already satisfied: requests[socks]>=2.12.0 in /opt/conda/lib/python3.8/site-packages (from gdown==3.13.0->-r requirements.txt (line 1)) (2.25.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from gdown==3.13.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from gdown==3.13.0->-r requirements.txt (line 1)) (4.61.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from gdown==3.13.0->-r requirements.txt (line 1)) (3.0.12)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torch==1.8.1->-r requirements.txt (line 2)) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.8.1->-r requirements.txt (line 2)) (3.10.0.0)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /opt/conda/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements.txt (line 3)) (0.0.8)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements.txt (line 3)) (0.10.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements.txt (line 3)) (20.9)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements.txt (line 3)) (0.0.45)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements.txt (line 3)) (2021.8.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown==3.13.0->-r requirements.txt (line 1)) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown==3.13.0->-r requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown==3.13.0->-r requirements.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown==3.13.0->-r requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown==3.13.0->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers==4.6.1->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.6.1->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.6.1->-r requirements.txt (line 3)) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Night Vale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Welcome to Night Vale\" is a podcast presented as a radio show for the fictional town of Night Vale, reporting on the strange events that occur within it. It was created in 2012 by Joseph Fink and Jeffrey Cranor, and is full of Lovecraftian cosmic horror with a comical twist.\n",
    "\n",
    "The host of \"Welcome to Night Vale\" is Cecil Baldwin, who is played by American voice actor Cecil Palmer.\n",
    "\n",
    "In this notebook, we will be training a language model to generate fake podcast transcripts from the show using data collected from https://cecilspeaks.tumblr.com/, which contains transcripts for 191 episodes of the show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listen to the podcast\n",
    "\n",
    "For the curious, you can listen to an episode here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/due3u22Licw\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f339a0e75e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://www.youtube.com/embed/due3u22Licw\", width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oy2sFY8eNpkM"
   },
   "source": [
    "## Get the data\n",
    "\n",
    "To collect the transcripts, I wrote a webscraper using scrapy (https://scrapy.org/) and stored the files in a google drive to be downloaded in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2569,
     "status": "ok",
     "timestamp": 1625114542648,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "fzB-JSZVIQxN",
    "outputId": "c6fddf8d-7b23-417f-c887-6fa3a3e11200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1szUGhMsH9SFF52AZKRse_gZs7zB7ke8F\n",
      "To: /home/jovyan/welcome-to-nightvale/wtnv.zip\n",
      "100%|██████████████████████████████████████| 1.24M/1.24M [00:00<00:00, 1.87MB/s]\n",
      "Archive:  wtnv.zip\n",
      "  inflating: transcripts/1-pilot     \n",
      "  inflating: transcripts/2-glow-cloud  \n",
      "  inflating: transcripts/3-station-management  \n",
      "  inflating: transcripts/4-pta-meeting  \n",
      "  inflating: transcripts/5-the-shape-in-grove-park  \n",
      "  inflating: transcripts/6-the-drawbridge  \n",
      "  inflating: transcripts/7-history-week  \n",
      "  inflating: transcripts/8-the-lights-in-radon-canyon  \n",
      "  inflating: transcripts/9-pyramid   \n",
      "  inflating: transcripts/10-feral-dogs  \n",
      "  inflating: transcripts/11-wheat-amp-wheat-by-products  \n",
      "  inflating: transcripts/12-the-candidate  \n",
      "  inflating: transcripts/13-a-story-about-you  \n",
      "  inflating: transcripts/14-the-man-in-the-tan-jacket  \n",
      "  inflating: transcripts/15-street-cleaning-day  \n",
      "  inflating: transcripts/16-the-phone-call  \n",
      "  inflating: transcripts/17-valentine  \n",
      "  inflating: transcripts/18-the-traveler  \n",
      "  inflating: transcripts/19a-the-sandstorm  \n",
      "  inflating: transcripts/19b-the-sandstorm  \n",
      "  inflating: transcripts/20-poetry-week  \n",
      "  inflating: transcripts/21-a-memory-of-europe  \n",
      "  inflating: transcripts/22-the-whispering-forest  \n",
      "  inflating: transcripts/23-eternal-scouts  \n",
      "  inflating: transcripts/24-the-mayor  \n",
      "  inflating: transcripts/25-one-year-later  \n",
      "  inflating: transcripts/26-faceless-old-woman  \n",
      "  inflating: transcripts/27-first-date  \n",
      "  inflating: transcripts/28-summer-reading-program  \n",
      "  inflating: transcripts/29-subway   \n",
      "  inflating: transcripts/30-dana     \n",
      "  inflating: transcripts/31-a-blinking-light-up-on-the-mountain  \n",
      "  inflating: transcripts/32-yellow-helicopters  \n",
      "  inflating: transcripts/33-cassette  \n",
      "  inflating: transcripts/34-a-beautiful-dream  \n",
      "  inflating: transcripts/35-lazy-day  \n",
      "  inflating: transcripts/36-missing  \n",
      "  inflating: transcripts/37-the-auction  \n",
      "  inflating: transcripts/38-orange-grove  \n",
      "  inflating: transcripts/39-the-woman-from-italy  \n",
      "  inflating: transcripts/40-the-deft-bowman  \n",
      "  inflating: transcripts/41-walk     \n",
      "  inflating: transcripts/42-numbers  \n",
      "  inflating: transcripts/43-visitor  \n",
      "  inflating: transcripts/44-cookies  \n",
      "  inflating: transcripts/45-a-story-about-them  \n",
      "  inflating: transcripts/46-parade-day  \n",
      "  inflating: transcripts/47-company-picnic  \n",
      "  inflating: transcripts/48-renovations  \n",
      "  inflating: transcripts/49-old-oak-doors-part-a  \n",
      "  inflating: transcripts/49-old-oak-doors-part-b  \n",
      "  inflating: transcripts/50-capital-campaign  \n",
      "  inflating: transcripts/51-rumbling  \n",
      "  inflating: transcripts/52-the-retirement-of-pamela-winchell  \n",
      "  inflating: transcripts/53-the-september-monologues  \n",
      "  inflating: transcripts/54-a-carnival-comes-to-town  \n",
      "  inflating: transcripts/55-the-university-of-what-it-is  \n",
      "  inflating: transcripts/56-homecoming  \n",
      "  inflating: transcripts/57-the-list  \n",
      "  inflating: transcripts/58-monolith  \n",
      "  inflating: transcripts/59-antiques  \n",
      "  inflating: transcripts/60-water-failure  \n",
      "  inflating: transcripts/61-briney-depths  \n",
      "  inflating: transcripts/62-hatchets  \n",
      "  inflating: transcripts/63-there-is-no-part-1-part-2  \n",
      "  inflating: transcripts/64-we-must-give-praise  \n",
      "  inflating: transcripts/65-voicemail  \n",
      "  inflating: transcripts/66-worms    \n",
      "  inflating: transcripts/67-best-of  \n",
      "  inflating: transcripts/68-faceless-old-women  \n",
      "  inflating: transcripts/69-fashion-week  \n",
      "  inflating: transcripts/70a-taking-flight  \n",
      "  inflating: transcripts/70b-review  \n",
      "  inflating: transcripts/71-the-registry-of-middle-school-crushes  \n",
      "  inflating: transcripts/72-well-of-night  \n",
      "  inflating: transcripts/73-triptych  \n",
      "  inflating: transcripts/74-civic-changes  \n",
      "  inflating: transcripts/75-through-the-narrow-place  \n",
      "  inflating: transcripts/76-an-epilogue  \n",
      "  inflating: transcripts/77-a-stranger  \n",
      "  inflating: transcripts/78-cooking-stuff-thanksgiving-special  \n",
      "  inflating: transcripts/79-lost-in-the-mail  \n",
      "  inflating: transcripts/80-a-new-sheriff-in-town  \n",
      "  inflating: transcripts/81-after-3327  \n",
      "  inflating: transcripts/82-skating-rink  \n",
      "  inflating: transcripts/83-one-normal-town  \n",
      "  inflating: transcripts/84-past-time  \n",
      "  inflating: transcripts/85-april-monologues  \n",
      "  inflating: transcripts/86-standing-and-breathing  \n",
      "  inflating: transcripts/87-the-trial-of-hiram-mcdaniels  \n",
      "  inflating: transcripts/88-things-fall-apart  \n",
      "  inflating: transcripts/89-whos-a-good-boy-part-1  \n",
      "  inflating: transcripts/90-whos-a-good-boy-part-2  \n",
      "  inflating: transcripts/91-the-1237  \n",
      "  inflating: transcripts/92-if-he-had-lived  \n",
      "  inflating: transcripts/93-big-sister  \n",
      "  inflating: transcripts/94-all-right  \n",
      "  inflating: transcripts/95-zookeeper  \n",
      "  inflating: transcripts/96-negotiations  \n",
      "  inflating: transcripts/97-josefina  \n",
      "  inflating: transcripts/98-flight   \n",
      "  inflating: transcripts/99-michigan  \n",
      "  inflating: transcripts/100-toast   \n",
      "  inflating: transcripts/101-guidelines-for-disposal  \n",
      "  inflating: transcripts/102-love-is-a-shambling-thing  \n",
      "  inflating: transcripts/103-ash-beach  \n",
      "  inflating: transcripts/104-the-hierarchy-of-angels  \n",
      "  inflating: transcripts/105-what-happened-at-the-smithwick-house  \n",
      "  inflating: transcripts/106-filings  \n",
      "  inflating: transcripts/107-the-missing-sky  \n",
      "  inflating: transcripts/108-cal     \n",
      "  inflating: transcripts/109-a-story-about-huntokar  \n",
      "  inflating: transcripts/110-matryoshka  \n",
      "  inflating: transcripts/111-summer-2017-night-vale-usa  \n",
      "  inflating: transcripts/112-citizen-spotlight  \n",
      "  inflating: transcripts/113-niecelet  \n",
      "  inflating: transcripts/114-council-member-flynn-part-1  \n",
      "  inflating: transcripts/115-council-member-flynn-part-2  \n",
      "  inflating: transcripts/116-council-member-flynn-part-3  \n",
      "  inflating: transcripts/117-egemony-part-1-canadian-club  \n",
      "  inflating: transcripts/118-egemony-part-2-the-cavelands  \n",
      "  inflating: transcripts/119-egemony-part-3-love-among-other-things-is-all-you-need  \n",
      "  inflating: transcripts/120-all-smiles-eve  \n",
      "  inflating: transcripts/121-a-story-of-love-and-horror-part-1-barks  \n",
      "  inflating: transcripts/122-a-story-of-love-and-horror-part-2-spire  \n",
      "  inflating: transcripts/123-a-story-of-love-and-horror-part-3-frances  \n",
      "  inflating: transcripts/124-a-door-ajar-part-1  \n",
      "  inflating: transcripts/125-a-door-ajar-part-2  \n",
      "  inflating: transcripts/126-a-door-ajar-part-3  \n",
      "  inflating: transcripts/127-a-matter-of-blood-part-1  \n",
      "  inflating: transcripts/128-a-matter-of-blood-part-2  \n",
      "  inflating: transcripts/129-a-matter-of-blood-part-3  \n",
      "  inflating: transcripts/130-a-story-about-us  \n",
      "  inflating: transcripts/131-brought-to-you-by-kelloggs  \n",
      "  inflating: transcripts/132-bedtime-story  \n",
      "  inflating: transcripts/133-are-you-sure  \n",
      "  inflating: transcripts/134-fall-football-preview  \n",
      "  inflating: transcripts/135-the-mudstone-abyss-part-1  \n",
      "  inflating: transcripts/136-the-mudstone-abyss-part-2  \n",
      "  inflating: transcripts/137-the-mudstone-abyss-part-3  \n",
      "  inflating: transcripts/138-harvest-time  \n",
      "  inflating: transcripts/139-the-birthday-of-lee-marvin  \n",
      "  inflating: transcripts/140-a-blood-stone-carol  \n",
      "  inflating: transcripts/141-save-dark-owl-records  \n",
      "  inflating: transcripts/142-ufo-sighting-report  \n",
      "  inflating: transcripts/143-pioneer-days  \n",
      "  inflating: transcripts/144-the-dreamer  \n",
      "  inflating: transcripts/145-the-veterans  \n",
      "  inflating: transcripts/146-the-birthday-of-lee-marvin  \n",
      "  inflating: transcripts/147-the-protester  \n",
      "  inflating: transcripts/148-the-broadcaster  \n",
      "  inflating: transcripts/149-the-general  \n",
      "  inflating: transcripts/150-the-birthday-of-lee-marvin  \n",
      "  inflating: transcripts/151-the-waterfall  \n",
      "  inflating: transcripts/152-the-great-golden-hand  \n",
      "  inflating: transcripts/153-the-heist-part-1  \n",
      "  inflating: transcripts/154-the-heist-part-2  \n",
      "  inflating: transcripts/155-the-heist-part-3  \n",
      "  inflating: transcripts/156-the-trouble-with-time  \n",
      "  inflating: transcripts/157-the-promise-of-time  \n",
      "  inflating: transcripts/158-the-battle-for-time  \n",
      "  inflating: transcripts/159-cat-show  \n",
      "  inflating: transcripts/160-the-weather  \n",
      "  inflating: transcripts/161-the-space-race  \n",
      "  inflating: transcripts/162-alpha   \n",
      "  inflating: transcripts/163-bravo   \n",
      "  inflating: transcripts/164-the-faceless-old-woman-live  \n",
      "  inflating: transcripts/165-charlie  \n",
      "  inflating: transcripts/166-delta   \n",
      "  inflating: transcripts/167-echo    \n",
      "  inflating: transcripts/168-secret-blotter  \n",
      "  inflating: transcripts/169-the-whittler  \n",
      "  inflating: transcripts/170-to-the-family-and-friends  \n",
      "  inflating: transcripts/171-go-to-the-mirror  \n",
      "  inflating: transcripts/172-return-of-the-obelisk  \n",
      "  inflating: transcripts/173-the-hundred-year-play  \n",
      "  inflating: transcripts/174-radio-jupiter  \n",
      "  inflating: transcripts/175-the-october-monologues  \n",
      "  inflating: transcripts/176-the-autumn-specter  \n",
      "  inflating: transcripts/177-bloody-laws-bloody-claws-the-murder-of-frank-chen  \n",
      "  inflating: transcripts/178-rattlesnake-rest  \n",
      "  inflating: transcripts/179-first-snow  \n",
      "  inflating: transcripts/180-u-view  \n",
      "  inflating: transcripts/181-cs      \n",
      "  inflating: transcripts/182-it-sticks-with-you  \n",
      "  inflating: transcripts/183-the-nephilim  \n",
      "  inflating: transcripts/184-the-fog  \n",
      "  inflating: transcripts/185-fair    \n",
      "  inflating: transcripts/186-the-many-lives-of-frank-chen  \n",
      "  inflating: transcripts/187-citizen-spotlight-the-spire  \n",
      "  inflating: transcripts/188-listener-questions  \n"
     ]
    }
   ],
   "source": [
    "# Delete the data if it already exists\n",
    "! rm -rf wtnv.zip transcripts\n",
    "\n",
    "# Download the data\n",
    "! gdown --id \"1szUGhMsH9SFF52AZKRse_gZs7zB7ke8F\"\n",
    "\n",
    "# Unzip the data\n",
    "! unzip wtnv.zip -d transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a peek at a transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just downloaded 191 podcast transcripts, which are listed above. Next we can inspect one of the files using `head`, which displays the first 10 lines of a file.\n",
    "\n",
    "If you would like to see the rest of the file, or more files, then feel free to explore the `transcripts/` directory on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1625114542648,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "JMQLEeLmNnog",
    "outputId": "8445d3bc-10b5-4c3d-a0fa-9dc1701a4935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - Pilot\n",
      "\n",
      "A friendly desert community, where the sun is hot, the moon is beautiful, and mysterious lights pass overhead while we all pretend to sleep. Welcome to Night Vale. \n",
      "\n",
      "Hello listeners. To start things off, I’ve been asked to read to read this brief notice. The City Council announces the opening of a new dog park at the corner of Earl and Summerset, near the Ralphs. They would like to remind everyone that dogs are not allowed in the dog park. People are not allowed in the dog park. It is possible you will see hooded figures in the dog park. Do not approach them. Do not approach the dog park. The fence is electrified and highly dangerous. Try not to look at the dog park and especially do not look for any period of time at the hooded figures. The dog park will not harm you.\n",
      "\n",
      "And now the news. Old Woman Josie, out near the car lot, says the Angels revealed themselves to her. Said they were ten feet tall, radiant, one of them was black. Said they helped her with various household chores. One of them changed  a light bulb for her, the porch light. She’s offering to sell the old light bulb, which has been touched by an angel (it was the black angel, if that sweetens the pot for anyone). If you’re interested, contact Old Woman Josie. She’s out near the car lot.\n",
      "\n",
      "A new man came in to town today.  Who is he? What does he want from us? Why his perfect and beautiful haircut? Why his perfect and beautiful coat? He says he is a scientist. Well, we have all been scientists at one point or another in our lives.  But why now? Why here? And just what does he plan to do with all those beakers and humming electrical instruments in that lab he’s renting, the one next to Big Rico’s Pizza. No one does a slice, like Big Rico. No one.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the data\n",
    "! head transcripts/1-pilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listen to the transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another iFrame with episode 1 in it so you can compare the recording to the transcript above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1625114542649,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "ARUXLQnHOSwM",
    "outputId": "dd429be8-49ff-4dcd-c6cc-3431c67e86e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/Ujksjzqrhys\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f339a0d9130>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/Ujksjzqrhys\", width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blPjgxcrPTLR"
   },
   "source": [
    "## Split the data into train/test sets\n",
    "\n",
    "Splitting data into different sets is a common operation for training any machine learning model. Because the model will learn from data, we need to set aside some data that we will not show the language model for evaluation. This helps us to detect overfitting.\n",
    "\n",
    "There is a trade-off between providing more training data, and avoiding overfitting. The more training data you provide, the better your model performs. But the smaller your test set, the more likely you are to overfit.\n",
    "\n",
    "For this project, we'll take the first 90% of the podcast transcripts and add them to a training set, while the remaining 10% are written to a test set. This decision is somewhat arbitrary, and you could argue for putting almost everything in the training set for this application.\n",
    "\n",
    "### How we chose to split the data\n",
    "\n",
    "There are 191 transcripts, so we can take the first 171 and pipe them to file which we'll call `train.txt`, and take the remaining 20 transcripts and pipe them to `test.txt`.\n",
    "\n",
    "In the cell below, I used `bash` command to concatenate the training + test text files automatically.\n",
    "\n",
    "### An unnecessary bash diversion\n",
    "\n",
    "If you wanna know how this works at a high level, here's a summary (but feel free to ignore it):\n",
    "\n",
    "- `ls` lists the files in the `transcripts` directory, `-t` orders the files in sequence.\n",
    "- Then we use `head` to grab the first 171 files for the training set, or in the latter case, we use `tail` to grab to last 20 files\n",
    "- Then `xargs` is a bit like a for loop. It runs `cat` over each file, which prints the file contents to stdout.\n",
    "- Then we pipe the file contents to the output file (`train.txt` or `test.txt`) using the `>` symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1625114543509,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "3wVZXcbKSsJC"
   },
   "outputs": [],
   "source": [
    "# Save the first 171 transcripts to train.txt\n",
    "! ls transcripts -t | head -n 171 | xargs -I {} cat transcripts/{} > train.txt\n",
    "\n",
    "# Save the last 20 transcripts to train.txt\n",
    "! ls transcripts -t | tail -n 20 | xargs -I {} cat transcripts/{} > test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this cell has been run, the files `train.txt` and `test.txt` should now exist. We will inspect them in the next step just to make sure we did it right, but you can always check them directly using the panel on the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the training + test data\n",
    "\n",
    "We can take a quick look at `train.txt` and `test.txt`, and count the number of lines in each. Alternatively open the files in a separate tab/window and look at them for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - Pilot\n",
      "\n",
      "A friendly desert community, where the sun is hot, the moon is beautiful, and mysterious lights pass overhead while we all pretend to sleep. Welcome to Night Vale. \n",
      "\n",
      "Hello listeners. To start things off, I’ve been asked to read to read this brief notice. The City Council announces the opening of a new dog park at the corner of Earl and Summerset, near the Ralphs. They would like to remind everyone that dogs are not allowed in the dog park. People are not allowed in the dog park. It is possible you will see hooded figures in the dog park. Do not approach them. Do not approach the dog park. The fence is electrified and highly dangerous. Try not to look at the dog park and especially do not look for any period of time at the hooded figures. The dog park will not harm you.\n",
      "\n",
      "And now the news. Old Woman Josie, out near the car lot, says the Angels revealed themselves to her. Said they were ten feet tall, radiant, one of them was black. Said they helped her with various household chores. One of them changed  a light bulb for her, the porch light. She’s offering to sell the old light bulb, which has been touched by an angel (it was the black angel, if that sweetens the pot for anyone). If you’re interested, contact Old Woman Josie. She’s out near the car lot.\n",
      "\n",
      "A new man came in to town today.  Who is he? What does he want from us? Why his perfect and beautiful haircut? Why his perfect and beautiful coat? He says he is a scientist. Well, we have all been scientists at one point or another in our lives.  But why now? Why here? And just what does he plan to do with all those beakers and humming electrical instruments in that lab he’s renting, the one next to Big Rico’s Pizza. No one does a slice, like Big Rico. No one.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 10 lines of train.txt\n",
    "! head train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169 - The Whittler\n",
      "\n",
      "[\n",
      "\n",
      "]\n",
      "\n",
      "Let us go then you and I,\n",
      "\n",
      "when the evening is spread out against the sky,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 10 lines of test.txt\n",
    "! head test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  23928 train.txt\n",
      "   3008 test.txt\n",
      "  26936 total\n"
     ]
    }
   ],
   "source": [
    "# Show the number of lines in train.txt and test.txt\n",
    "! wc -l train.txt test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split completed!\n",
    "\n",
    "Now we have successfully prepared our dataset for training, we are ready to start building our fine-tuning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO4xXZR7Tgl-"
   },
   "source": [
    "## Fine-tuning a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are going to fine-tune a language model using the `huggingface` (https://huggingface.co) team's excellent `transformers` package.\n",
    "\n",
    "`transformers` allows you to download, use and manipulate a wide range of language models trained for different applications. They provide tools for downloading model weights, using them for inference, training from scratch and fine-tuning. It's also pretty easy to use.\n",
    "\n",
    "More information about `transformers` is available here: https://huggingface.co/transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2\n",
    "\n",
    "We are going to be using the `GPT-2` language model, which was pre-trained by Open AI who first published it in 2019. You can read more about `GPT-2` here: https://openai.com/blog/gpt-2-1-5b-release/\n",
    "\n",
    "We are going to use the `GPT2Tokenizer`, and `GPT2LMHeadModel` tools.\n",
    "- A `tokenizer` breaks up a string of text into words or word-fragments in a way that the language model understands.\n",
    "- An `LMHeadModel` exposes the Language Model of the GPT-2 model, which makes text generation possible.\n",
    "\n",
    "There are other kinds of Language Models which are specialised for different tasks. You can find a bunch of summaries for all the models supported by `transformers` here: https://huggingface.co/transformers/model_summary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2256,
     "status": "ok",
     "timestamp": 1625114545764,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "2sDObY9mTij-"
   },
   "outputs": [],
   "source": [
    "# Import transformers stuff\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wnRlvx0TtP3"
   },
   "source": [
    "### Load pre-trained GPT-2 models\n",
    "\n",
    "Now that we have imported the library, we need to download the `GPT-2` model weights so we can start using them. The cell below downloads them straight to disk from `huggingface`'s servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2065,
     "status": "ok",
     "timestamp": 1625114547828,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "qvHHwRgkTpgC"
   },
   "outputs": [],
   "source": [
    "# Load a tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load GPT2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaY1NYVEUAG6"
   },
   "source": [
    "## Load our training datasets\n",
    "\n",
    "Next, we need to load our train + test datasets into a format that the `GPT-2` can use. Notice we pass the `train.txt` and `test.txt` files here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1625114547831,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "PdxI_4ykTzGu",
    "outputId": "b383165a-1be5-4591-96eb-46706e386211"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer, file_path='train.txt', block_size=128\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(tokenizer=tokenizer, file_path='test.txt', block_size=128)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False, # GPT-2 only supports mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters\n",
    "\n",
    "In the cell below, we set hyperparameters which will affect how our model will run. This table gives a summary of what each hyperparameter does, and how it affects the model if you set it too high or too low.\n",
    "\n",
    "| Hyperparameter name | What it does | What if its too big | What if its too small |\n",
    "| --- | --- | --- | --- |\n",
    "| `num_train_epochs` | The number of times the language model will review the text during training. | The model takes a long time to train. | The model will not learn much and the results will lean towards the pre-trained model instead of the new data you have provided. |\n",
    "| `per_device_train_batch_size` | The size of the batches in which the training loop will consume the training data. | You will run out of GPU memory. | The train will take a long time. |\n",
    "| `per_device_eval_batch_size` | The size of the batches in which the training loop will consume the test data. | You will run out of GPU memory. | The model evaluation will take a long time. |\n",
    "| `eval_steps` | Sets how frequently the model will run an evaluation step (that is, it will run inference on the test data to check for overfitting). | You will run evaluation too often which will slow down the training loop. | You may overfit by a lot before you are able to notice. |\n",
    "| `save_steps` | Sets how frequently the model will write checkpoints to disk in order to save its progress. | If your train fails you will need to re-run a lot of computation. | You will create too many checkpoints and run out of disk space. |\n",
    "\n",
    "In actuality, there are _many_ more hyperparameters than this. You can read more about them here: https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments\n",
    "\n",
    "Now we set the hyperparameter values in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1625114547833,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "PFO4mAxQUk4z"
   },
   "outputs": [],
   "source": [
    "num_train_epochs = 1  ## You can set this higher, but it will take longer\n",
    "\n",
    "# Mostly these can be left as they are - feel free to play with them and see what happens however\n",
    "per_device_train_batch_size = 18\n",
    "per_device_eval_batch_size = 16\n",
    "eval_steps = 400\n",
    "save_steps = 800\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='wtnv_model',                                 # The folder where we save the model\n",
    "    overwrite_output_dir=True,                               # overwrite the content of the output directory\n",
    "    num_train_epochs=num_train_epochs,                       # number of training epochs\n",
    "    per_device_train_batch_size=per_device_train_batch_size, # batch size for training\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,   # batch size for evaluation\n",
    "    eval_steps=eval_steps,                                   # Number of update steps between two evaluations.\n",
    "    save_steps=save_steps,                                   # after # steps model is saved\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now we have done all that work, we can finally train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3637,
     "status": "ok",
     "timestamp": 1625114551464,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "PxvZT5sLUt9n"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254/254 01:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialise a Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Run the model train\n",
    "trainer.train()\n",
    "\n",
    "# Save the model when it's done\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvzyTNLVYuBB"
   },
   "source": [
    "## Generate some text\n",
    "\n",
    "Now that we have trained a model, it's time to load it into memory and run some text through it to see the results for ourself.\n",
    "\n",
    "In order to generate text, we need to do the following:\n",
    "\n",
    "- Load the trained model into memory\n",
    "- Feed some text to start the new transcript\n",
    "- Convert the text to a vector of token IDs that the model understands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1914,
     "status": "ok",
     "timestamp": 1625114872422,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "yfQ_S0dOYu5U",
    "outputId": "3cbb42ad-39a8-46ff-99e8-0cbca81ef048"
   },
   "outputs": [],
   "source": [
    "# Load the model into memory\n",
    "wtnv_model = GPT2LMHeadModel.from_pretrained(\"wtnv_model\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want the new transcript to start with different text, change the `seed_text` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1914,
     "status": "ok",
     "timestamp": 1625114872422,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "yfQ_S0dOYu5U",
    "outputId": "3cbb42ad-39a8-46ff-99e8-0cbca81ef048"
   },
   "outputs": [],
   "source": [
    "# Feed some text to start the new transcript\n",
    "seed_text = 'Welcome to Night Vale'  #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seed_ids` is a vector of IDs which represent the words in the source sentence.\n",
    "\n",
    "The `tokenizer` object provides a map which converts words to IDs and IDs back to words. The number of IDs should match the number of words in the `seed_text`, and if you use the same word more than once the IDs should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14618,   284,  5265, 31832]])\n"
     ]
    }
   ],
   "source": [
    "# Convert the sentence to a tensor of token IDs\n",
    "seed_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
    "print(seed_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can decode the `input_ids` using the `tokenizer.decode` methods as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Night Vale\n"
     ]
    }
   ],
   "source": [
    "# Convert the seed_ids back to text\n",
    "print(tokenizer.decode(seed_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we will actually generate text using the new model's `generate` method. There are a lot of interesting details between how sampling language models work, but the general idea is that the language model provides probabilities for next words given the words so far.\n",
    "\n",
    "Then you sample these probabilities using one of a number of strategies, and the way you sample these probabilities affects the text that comes out at the end.\n",
    "\n",
    "For more information on how `generate` works, take a look at this blog post: https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69311,
     "status": "ok",
     "timestamp": 1625115090790,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "a7seigeZY1Tt",
    "outputId": "733a0fe4-e241-412e-d025-de2c069818aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55min 17s, sys: 2min 42s, total: 57min 59s\n",
      "Wall time: 7min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = wtnv_model.generate(\n",
    "    \n",
    "    seed_ids,       # The seed text IDs\n",
    "    do_sample=True,\n",
    "    \n",
    "    # Tweak these values and see what they do:\n",
    "    max_length=500,\n",
    "    top_k=0,\n",
    "    top_p=0.92,\n",
    "    temperature=0.9\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1625115662924,
     "user": {
      "displayName": "Caleb Moses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihgFuNP61y8rJEFwDnq_sK45X5AfM6kOcDELg7=s64",
      "userId": "05614899954161823090"
     },
     "user_tz": -720
    },
    "id": "2m5f-tMJcD2F",
    "outputId": "bf58248f-b0e3-45ab-8da4-3cd34174a46a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Night Vale.\n",
      "\n",
      "More on this in a minute.\n",
      "\n",
      "SOMETHING:\n",
      "\n",
      "Night Vale is in trouble with another apocalyptic story that's brought you a good look at the missing people, and also some hope.\n",
      "\n",
      "Friday night's Night Vale Play House hosted by the non-profit Daily Welcome to Night Vale, which has planned another national carnival, this time in the middle of nowhere, at the site of the world-famous prison complex, and they're taking place downtown next to the Main Street Mall. Fun Fact: you just have to go where it isn't obvious to you.\n",
      "\n",
      "And then this afternoon at 6:00pm, Night Vale City Council will vote on what to do with the missing, the last couple of hours of life. As you know, folks with powers have been at the main ballroom of The City Hall, wondering what the whole thing is up to. Well, that’s not a good thing. That’s not a good idea. The city’s new mayor, Dolores Umbridge, told us that she’s gonna vote no on bringing the Night Vale government to town. She will instead say that she wants to go to the Catskill Mountains to get better animal husbandry, and that they will see her through some low-hanging fruit boxes. She wants to do her bit to stop this dangerous rampage, which is exactly what we do now. Night Vale City Council could not be reached for comment. But we do know that one person has survived the night, and that two of her old cubs are fighting on top of her. That’s absolutely exciting. And that’s a heads-up.\n",
      "\n",
      "Of course, that doesn’t mean that the mayor is happy. Night Vale is, on the surface, a fun city. Yes, many people have enjoyed traveling in town. Many people are excited to visit places like the Little Ice Box and the North Woods, where they can experience the natural beauty of the frozen earth, hear the soaring voices of life, and experience nature and the beautiful language of life, and eat tasty meat and fish from the mouths of monsters who see no human life. But there is a downside to those things, and that is that other people do not look after the bodies of those people. Night Vale will also, as of this writing, no longer have its own massive body count. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's one I prepared earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can download a 20 epoch model train that I ran separately on the same data. It took about 20 minutes to train on an Nvidia RTX 2080 Ti GPU. Because this one has been trained considerably longer, the results should be noticably different.\n",
    "\n",
    "The model weights are about 500MB zipped, so downloading it will just a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1igtAPMjk-fDFCSC2BS_W3FyPyJSK2FSJ\n",
      "To: /home/jovyan/welcome-to-nightvale/wtnv20_model.zip\n",
      "463MB [00:12, 36.4MB/s] \n",
      "Archive:  wtnv20_model.zip\n",
      "  inflating: wtnv20_model/config.json  \n",
      "  inflating: wtnv20_model/pytorch_model.bin  \n",
      "  inflating: wtnv20_model/training_args.bin  \n"
     ]
    }
   ],
   "source": [
    "# Download the model\n",
    "! gdown --id \"1igtAPMjk-fDFCSC2BS_W3FyPyJSK2FSJ\"\n",
    "\n",
    "# Unzip the model\n",
    "! unzip wtnv20_model.zip -d wtnv20_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can quickly re-initialise the new model and generate another transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load the 20-epoch model into memory\n",
    "wtnv_model = GPT2LMHeadModel.from_pretrained(\"wtnv20_model\", local_files_only=True)\n",
    "\n",
    "# Feed some text to start the new transcript\n",
    "seed_text = 'Welcome to Night Vale'  #@param {type:\"string\"}\n",
    "\n",
    "# Convert the sentence to a tensor of token IDs\n",
    "seed_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
    "\n",
    "output = wtnv_model.generate(\n",
    "    \n",
    "    seed_ids,       # The seed text IDs\n",
    "    do_sample=True,\n",
    "    \n",
    "    # Tweak these values and see what they do:\n",
    "    max_length=1000,\n",
    "    top_k=0,\n",
    "    top_p=0.92,\n",
    "    temperature=0.9\n",
    "    \n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuff to do next:\n",
    "\n",
    "- Google some of the characters and places to see if they exist in the world of Welcome to Night Vale or not\n",
    "- Take some time to generate new texts with different variables and get a feel for how the output changes.\n",
    "- Make some comments on the weaknesses of the model output.\n",
    "- See if you can make a transcript that you think is pretty good\n",
    "- See if you can make a transcript that is pretty terrible"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJlPkF4enmF1X56KJwWuhj",
   "collapsed_sections": [],
   "name": "Fine-tuning a language model with huggingface.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
